import argparse
import torch
import os
import json
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

from PIL import Image
import math
import numpy as np
# import debugpy
import re


def split_list(lst, n):
    """Split a list into n (roughly) equal-sized chunks"""
    chunk_size = math.ceil(len(lst) / n)  # integer division
    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]


def get_chunk(lst, n, k):
    chunks = split_list(lst, n)
    return chunks[k]


def make_conv_rm(prompt, chosen, rejected, has_image=True):
    prompt_template = (
        "You are a highly skilled and impartial evaluator tasked with comparing two responses generated by a Large Multimodal Model for a given question. "
        "- Start with a thorough, side-by-side comparative analysis enclosed within <think> and </think> tags. A tie is not permitted; you must choose a better option.\n\n"
        "- Conclude with a single numeric choice enclosed within <answer> and </answer> tags:\n"
        "  - Output \"1\" if Response 1 is better.\n"
        "  - Output \"2\" if Response 2 is better.\n\n"
        "###### **Input:**  \n"
        "###### [Question]:\n{question}  \n\n"
        "###### [Response 1]:\n{answer1}  \n\n"
        "###### [Response 2]:\n{answer2}  \n\n"
        "###### **Output Format (strictly follow):**  \n"
        "<think>Your detailed comparative analysis goes here</think><answer>1/2</answer>"
    )
    formatted_prompt = prompt_template.format(question=prompt, answer1=chosen, answer2=rejected)
    return formatted_prompt

import torch.nn as nn
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoConfig
from qwen_vl_utils import process_vision_info
from safetensors import safe_open
import torch.nn.functional as F

class CustomScore(nn.Module):

    def __init__(self, cfg, n_labels: int):
        super().__init__()
        self.dense = nn.Linear(cfg.hidden_size, cfg.hidden_size)
        # use same dropout as attention dropout
        self.dropout = nn.Dropout(cfg.attention_dropout)
        self.out_proj = nn.Linear(cfg.hidden_size, n_labels)

    def forward(self, hidden_states: torch.Tensor, **kwargs):
        hidden_states = hidden_states.to(self.dense.weight.device)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.dense(hidden_states)
        hidden_states = torch.tanh(hidden_states)
        hidden_states = self.dropout(hidden_states)
        output = self.out_proj(hidden_states)
        return output

class CustomRewardModel(Qwen2_5_VLForConditionalGeneration):
    def __init__(self, config):
        super().__init__(config)
        self.score = CustomScore(config, config.num_labels)

def eval_model(args):
    # Model
    device = "cuda"

    config = AutoConfig.from_pretrained(args.model_path)
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        args.model_path, 
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="flash_attention_2",
        config=config
    )

    processor = AutoProcessor.from_pretrained(args.model_path, max_pixels=1003520 // 2)


    model.eval()

    questions = []
    with open(args.question_file, 'r') as file:
        questions.extend(json.load(file))
    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
    answers_file_with_chunks = f"{args.answers_file.rsplit('.', 1)[0]}_chunk{args.chunk_idx}_of_{args.num_chunks}.jsonl"
    existing_ids = set()
    try:
        with open(answers_file_with_chunks, 'r') as file:
            for line in file:
                answer = json.loads(line)
                existing_ids.add(answer.get("ID"))
    except FileNotFoundError:
        print(f"No existing file found: {answers_file_with_chunks}, proceeding with empty set of existing IDs.")

    questions = [q for q in questions if q.get("ID") not in existing_ids]
    print(f'saving all the answers to {answers_file_with_chunks}')
    answers_file = os.path.expanduser(answers_file_with_chunks)
    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "a+")

    index, cnt_images = 0, []
    import random
    for line in tqdm(questions, total=len(questions)):
        video_file = line.get("video", None)
        image_file = line.get("Image", None)
    
        rewards = []
        critic_texts = []
        rand_num = random.random()
        if line['Better'] == "Output2":
            chosen, rejected = line['Output2'], line['Output1']
        else:
            chosen, rejected = line['Output1'], line['Output2']
        
        
        qs = make_conv_rm(line["Text"], chosen, rejected, video_file or image_file)
        file_mm = os.path.join(args.image_folder, image_file)
        messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": file_mm,
                        },
                        {"type": "text", "text": qs},
                    ],
                }
            ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
            )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda")
        
        critic_reward = []
        
        if not args.use_gt_critic and not args.wo_critic:
            with torch.inference_mode():
                generated_ids = model.generate(**inputs, max_new_tokens=4096)
            generated_ids_trimmed = [
                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )
                                        
            critic = output_text[0]

        try:
            content_match = re.search(r"<answer>(.*?)</answer>", critic, re.DOTALL)
            if content_match:
                student_answer = content_match.group(1).strip()
            else:
                # 如果没有找到 <answer> 标签，则尝试从字符串中找最后一个 '0' 或 '1'
                last_match = re.findall(r"[12]", critic)
                student_answer = last_match[-1] if last_match else None

            if student_answer is not None and float(student_answer) == 1:
                rewards = [1, 0]
            else:
                rewards = [0, 1]
        except:
            print('cannot find <answer></answer> tag')
            rewards = [0, 1]
    
        line['qs'] = qs
    
        index += 1
        line['rewards'] = rewards
        line['critic'] = critic
        ans_file.write(json.dumps(line) + "\n")
        ans_file.flush()
    ans_file.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # sampling strategy follows simpo
    # nohup python examples/scripts/reward_model/vl_reward_bench.py --num-chunks 4 --chunk-idx 3 >> examples/scripts/reward_model/logs/qwenvl25_7b_ciritc_lr1e_5_bsz128_wo_critic_newhead_0_4.log 2>&1 &
    # parser.add_argument("--rm-model-path", type=str, default="../model/alignment/baseline_promptv3")
    parser.add_argument("--model-path", type=str, default="yifanzhang114/R1-Reward")
    parser.add_argument("--image-folder", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/data/")
    parser.add_argument("--question-file", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/data/all_data.json")
    parser.add_argument("--answers-file", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/outputs/r1-rweard.jsonl")
    parser.add_argument("--conv-mode", type=str, default="qwen_1_5")
    parser.add_argument("--num-chunks", type=int, default=30)
    parser.add_argument("--chunk-idx", type=int, default=2)
    parser.add_argument("--temperature", type=float, default=0.)
    parser.add_argument("--top_p", type=float, default=None)
    parser.add_argument("--num_beams", type=int, default=1)
    parser.add_argument("--num_critic", type=int, default=1)
    parser.add_argument("--max_new_tokens", type=int, default=4096)

    parser.add_argument("--use_gt_critic", type=bool, default=False)
    parser.add_argument("--wo_critic", type=bool, default=False)

    parser.add_argument(
        "--test-prompt",
        type=str,
        default="",
    )
    args = parser.parse_args()
    print(args)

    eval_model(args)