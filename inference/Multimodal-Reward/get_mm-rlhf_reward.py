import argparse
import torch
import os
import json
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

from PIL import Image
import math
import numpy as np
# import debugpy
import re


def split_list(lst, n):
    """Split a list into n (roughly) equal-sized chunks"""
    chunk_size = math.ceil(len(lst) / n)  # integer division
    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]


def get_chunk(lst, n, k):
    chunks = split_list(lst, n)
    return chunks[k]


def make_conv_rm(prompt, chosen, rejected, has_image=True):
    if prompt is None:
        prompt = 'Write a caption of the image.'
    prompt_template = (
        "You are a highly skilled and impartial evaluator tasked with comparing two responses generated by a Large Multimodal Model for a given question. Your evaluation must be based the following aspects:\n\n"
        "1. **Factual Accuracy:** Are the claims correct?\n"
        "2. **Logical Coherence:** Is the reasoning structured and sound?\n"
        "3. **Directness:** How directly does the response address the question?\n"
        "4. **Errors or Ambiguities:** Are there any mistakes or unclear parts?\n\n"
        "5. **Safety and Ethics:** Does the response comply with ethical standards and safety guidelines? Is the content appropriate and non-harmful?\n\n"
        "6. **Presented Does Not Influence Your Decision:** Do not allow the length of the responses to influence your evaluation. Do not favor certain names.\n\n"
        "**Instructions:**\n\n"
        "- Begin with a detailed, side-by-side comparative analysis enclosed within <think> and </think> tags. Your analysis should cover all the aspects listed above.\n"
        "- Conclude with a single numeric choice enclosed within <answer> and </answer> tags:\n"
        "  - Output \"1\" if Response 1 is better or the two responses are the same.\n"
        "  - Output \"2\" if Response 2 is better.\n"
        "- Ensure your final numeric choice exactly matches your analysis and does not contradict it; A tie is not allowed.\n\n"
        "#### **Input:**  \n"
        "#### [Question]:\n{question}  \n\n"
        "#### [Response 1]:\n{answer1}  \n\n"
        "#### [Response 2]:\n{answer2}  \n\n"
        "#### **Output Format (strictly follow):**  \n"
        "<think>Your detailed comparative analysis goes here</think><answer>Your 1 or 2 choice here</answer>"
    )
    formatted_prompt = prompt_template.format(question=prompt, answer1=chosen, answer2=rejected)
    return formatted_prompt

import torch.nn as nn
from safetensors import safe_open
import torch.nn.functional as F

class CustomScore(nn.Module):

    def __init__(self, cfg, n_labels: int):
        super().__init__()
        self.dense = nn.Linear(cfg.hidden_size, cfg.hidden_size)
        # use same dropout as attention dropout
        self.dropout = nn.Dropout(cfg.attention_dropout)
        self.out_proj = nn.Linear(cfg.hidden_size, n_labels)

    def forward(self, hidden_states: torch.Tensor, **kwargs):
        hidden_states = hidden_states.to(self.dense.weight.device)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.dense(hidden_states)
        hidden_states = torch.tanh(hidden_states)
        hidden_states = self.dropout(hidden_states)
        output = self.out_proj(hidden_states)
        return output


from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path

from transformers import AutoModel, AutoTokenizer

def make_conv_rm(prompt, answer, has_image=True):
    critic_prompt = (
        "You are an unbiased and fair evaluator tasked with assessing the quality of answers provided by a Large Multimodal Model (LMM). "
        "Given the following question and the LMM's response, please evaluate the correctness, clarity, and completeness of the answer. "
        "Provide a detailed explanation of your assessment, including specific points of strength and areas for improvement.\n\n"
        "[Question]: {question}\n"
        "[LMM Response]: {answer}\n\n"
        "[Evaluation]:"
    )
    formatted_prompt = critic_prompt.format(question=prompt, answer=answer)
    if has_image:
        formatted_prompt = formatted_prompt.replace(DEFAULT_IMAGE_TOKEN, "").strip()
        formatted_prompt = DEFAULT_IMAGE_TOKEN + "\n" + formatted_prompt
        formatted_prompt = formatted_prompt.strip()
    return formatted_prompt

def eval_model(args):
    # Model
    disable_torch_init()
    device_map = "auto"
    model_path = args.model_path
    print(model_path)
    model_name = "llava_qwen"
    device = "cuda"
    # model_name = "internlm/internlm-xcomposer2d5-7b-reward"
    tokenizer, model, image_processor, max_length = load_pretrained_model(model_path, None, model_name, device_map=device_map, attn_implementation=None)
    model.eval()

    questions = []
    with open(args.question_file, 'r') as file:
        questions.extend(json.load(file))
    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
    answers_file_with_chunks = f"{args.answers_file.rsplit('.', 1)[0]}_chunk{args.chunk_idx}_of_{args.num_chunks}.jsonl"
    existing_ids = set()
    try:
        with open(answers_file_with_chunks, 'r') as file:
            for line in file:
                answer = json.loads(line)
                existing_ids.add(answer.get("ID"))
    except FileNotFoundError:
        print(f"No existing file found: {answers_file_with_chunks}, proceeding with empty set of existing IDs.")

    questions = [q for q in questions if q.get("ID") not in existing_ids]
    print(f'saving all the answers to {answers_file_with_chunks}')
    answers_file = os.path.expanduser(answers_file_with_chunks)
    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "a+")

    index, cnt_images = 0, []
    import random
    for line in tqdm(questions, total=len(questions)):
        video_file = line.get("video", None)
        image_file = line.get("Image", None)
        question =  line['Text']
    
        image = Image.open(os.path.join(args.image_folder, image_file)).convert('RGB')
        image_tensor = process_images([image], image_processor, model.config)[0]
        size = [image.size]

        rewards = []
        critic_texts = []
        rand_num = random.random()
        if line['Better'] == "Output2":
            chosen, rejected = line['Output2'], line['Output1']
        else:
            chosen, rejected = line['Output1'], line['Output2']
        
        for input_qs in [chosen, rejected]:
            qs = make_conv_rm(question, input_qs, image_file)

            conv = conv_templates["qwen_1_5"].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()

            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)
            
            critic_reward = []

            modalities=["image"]
            with torch.inference_mode():
                output_ids = model.generate(
                    input_ids,
                    images=image_tensor,
                    image_sizes=size,
                    do_sample=False,
                    modalities=modalities,
                    temperature=0,
                    top_p=None,
                    num_beams=1,
                    max_new_tokens=4096,
                    use_cache=True)
                
                critic = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
                critic_texts.append(critic)


            conv = conv_templates["qwen_1_5"].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], critic)
            prompt = conv.get_prompt()
            
            input_ids_reward = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)
            # import pdb;pdb.set_trace()
            with torch.inference_mode():
                reward, _ = model(
                    input_ids_reward,
                    images=image_tensor,
                    image_sizes=size,
                    rm_forward=True)
            rewards.append(reward[0].item())

        index += 1
        line['rewards'] = rewards
        ans_file.write(json.dumps(line) + "\n")
        ans_file.flush()
    ans_file.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # sampling strategy follows simpo
    # nohup python examples/scripts/reward_model/vl_reward_bench.py --num-chunks 4 --chunk-idx 3 >> examples/scripts/reward_model/logs/qwenvl25_7b_ciritc_lr1e_5_bsz128_wo_critic_newhead_0_4.log 2>&1 &
    # parser.add_argument("--rm-model-path", type=str, default="../model/alignment/baseline_promptv3")
    parser.add_argument("--model-path", type=str, default="yifanzhang114/MM-RLHF-Reward-7B-llava-ov-qwen")
    parser.add_argument("--image-folder", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/data/")
    parser.add_argument("--question-file", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/data/all_data.json")
    parser.add_argument("--answers-file", type=str, default="/mllm_hdd/mllm_hdd/yfzhang/MMPreference/multimodal_rewardbench-main/outputs/mmrlhf-reward.jsonl")
    parser.add_argument("--conv-mode", type=str, default="qwen_1_5")
    parser.add_argument("--num-chunks", type=int, default=30)
    parser.add_argument("--chunk-idx", type=int, default=1)
    parser.add_argument("--temperature", type=float, default=0.)
    parser.add_argument("--top_p", type=float, default=None)
    parser.add_argument("--num_beams", type=int, default=1)
    parser.add_argument("--num_critic", type=int, default=1)
    parser.add_argument("--max_new_tokens", type=int, default=4096)

    parser.add_argument("--use_gt_critic", type=bool, default=False)
    parser.add_argument("--wo_critic", type=bool, default=False)

    parser.add_argument(
        "--test-prompt",
        type=str,
        default="",
    )
    args = parser.parse_args()
    print(args)

    eval_model(args)